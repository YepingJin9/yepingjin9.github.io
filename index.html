<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Yeping Jin</title>
  <meta name="description" content="Yeping Jin — Systems Engineering Ph.D. student at Boston University. Optimization, algorithms, and robust ML." />
  <style>
    :root{--bg:#fff;--fg:#111;--muted:#555;--link:#1a73e8;--card:#f6f7f9}
    @media (prefers-color-scheme:dark){:root{--bg:#0e1116;--fg:#e6edf3;--muted:#a0acb7;--link:#6ea8fe;--card:#141a21}}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:18px/1.65 -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Helvetica,Arial}
    a{color:var(--link);text-decoration:none}
    a:hover{text-decoration:underline}
    .wrap{max-width:980px;margin:0 auto;padding:28px}
    header{display:grid;grid-template-columns:1.1fr 320px;gap:28px;align-items:start;margin-top:10px}
    h1{font-size:46px;line-height:1.15;margin:8px 0}
    .hero p{margin:12px 0}
    .links{margin-top:10px}
    .links a{margin-right:16px}
    .photo{width:100%;height:320px;border-radius:8px;background:#dbe3ef center/contain no-repeat url('Yeping.jpg');border:1px solid #0001}
    .center-links {text-align: center;font-size: 18px;}

    section{margin-top:48px}
    h2{font-size:30px;margin:0 0 16px 0}

    .pub{display:grid;grid-template-columns:1fr;gap:12px;margin:18px 0 28px 0}
    .pub-title{font-weight:700}
    .pub-venue{font-style:italic}
    .pub small{color:var(--muted)}
    details summary{cursor:pointer;color:var(--link)}

    .footer{margin:60px 0 20px;color:var(--muted);text-align:center;font-size:14px}
    @media(max-width:860px){header{grid-template-columns:1fr} .photo{height:220px}}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div class="hero">
        <h1>Yeping Jin</h1>
        <p>
          I am a 3rd year Systems Engineering Ph.D. student at 
          <a href="https://www.bu.edu/" target="_blank" rel="noopener">Boston University</a>, 
          working under the supervision of 
          <a href="https://sites.bu.edu/paschalidis/people/yannis-paschalidis/" target="_blank" rel="noopener">Professor Ioannis Ch. Paschalidis</a>.
          My interests lie in optimization and algorithm analysis, with applications to robust machine learning and decision-making systems.
        </p>
        <p>Previously, I completed an M.S. in Operations Research at Columbia University and an M.A./B.S. in Mathematics at UCLA. I enjoy turning elegant theory into practical systems.</p>
        <!-- Uncomment to show availability note -->
        <!-- <p style="color:#c62828;font-weight:700">I'm actively looking for research internship positions for Summer 2026.</p> -->
        <p class="links center-links">
          <a href="resume.pdf" target="_blank">Resume</a> /
          <a href="mailto:yepjin@bu.edu">Email</a> /
          <a href="https://github.com/YepingJin9" target="_blank" rel="noopener">GitHub</a> /
          <a href="https://www.linkedin.com/in/yepingjin/" target="_blank" rel="noopener">LinkedIn</a>
        </p>
      </div>
      <div class="photo" title="Add your portrait as portrait.jpg"></div>
    </header>

    <section id="research">
      <h2>Research</h2>
      <p>I'm broadly interested in optimization for learning, distributional robustness, and algorithmic foundations. Selected projects below.</p>

      <!-- Work #1 (Most recent) -->
      <div class="pub">
        <div class="pub-title">Distributionally Robust Token Optimization</div>
        <div><strong>Yeping Jin</strong>, Jiaming Hu, Ioannis Ch. Paschalidis</div>
        <div class="pub-venue">Preprint, 2025.</div>
        <div>
          We integrate distributionally robust learning into Reinforced Token Optimization (RTO) to harden large language models against structured shifts in math problem distributions. The method emphasizes worst-case token-level feedback and verifier-aware rewards to improve out-of-distribution robustness without sacrificing in-distribution accuracy.
        </div>
      </div>

      <!-- Work #2 -->
      <div class="pub">
        <div class="pub-title">Region-Aware Antibody Language Models for Paired VH–VL Sequences: CDR-Focused Pretraining Improves Binding Prediction</div>
        <div>
          <strong>Mahtab Talaei</strong>,
          <strong>Kenji C. Walker</strong>,
          <strong>Boran Hao</strong>,
          <strong>Eliot Jolley</strong>,
          <strong>Yeping Jin</strong>,
          <strong>Dima Kozakov</strong>,
          <strong>John Misasi</strong>,
          <strong>Sandor Vajda</strong>,
          <strong>Ioannis Ch. Paschalidis</strong>,
          <strong>Diane Joseph-McCarthy</strong>
        </div>
        <div class="pub-venue">Manuscript in preparation, 2025.</div>
        <div>
          We propose a region-aware pretraining strategy over paired heavy (VH) and light (VL) variable domains using ESM2-3B and a compact 600M ESM-C model. We compare whole-chain, CDR-focused, and hybrid masking strategies and show that CDR-focused training yields superior embeddings for binding-affinity prediction, matching or exceeding larger antibody-specific baselines while training only on paired sequences.
        </div>
        <details>
          <summary>Abstract</summary>
          <p>
            Antibodies are a leading class of biologics, yet their architecture with conserved framework regions and hypervariable complementarity-determining regions (CDRs) poses unique challenges for computational modeling. We present a region-aware pretraining strategy for paired heavy (VH) and light (VL) sequences in variable domains using ESM2-3B and ESM-C (600M) protein language models. We compare three masking strategies: whole-chain, CDR-focused, and a hybrid approach. Through evaluation on binding-affinity datasets spanning single-mutant panels and combinatorial mutants, we demonstrate that CDR-focused training produces superior embeddings for functional prediction. Notably, training only on VH-VL pairs proves sufficient, eliminating the need for massive unpaired pretraining that provides no measurable downstream benefit. Our compact 600M ESM-C model achieves state-of-the-art performance, matching or exceeding larger antibody-specific baselines. These findings establish a principled framework for antibody language models: prioritize paired sequences with CDR-aware supervision over scale and complex training curricula to achieve both computational efficiency and predictive accuracy.
          </p>
        </details>
        <div><a href="https://www.biorxiv.org/content/10.1101/2025.10.31.685149v1.full" target="_blank" rel="noopener">manuscript</a></div>
      </div>

      <!-- Work #3 -->
      <div class="pub">
        <div class="pub-title">Distributionally Robust Learning in Survival Analysis</div>
        <div><strong>Yeping Jin</strong>, Lauren A. Wise, Ioannis Ch. Paschalidis</div>
        <div class="pub-venue">arXiv, 2025.</div>
        <div>
          We develop distributionally robust formulations for survival analysis that improve stability and performance under covariate shift and heterogeneous subpopulations, with applications to clinical risk modeling.
        </div>
        <div>
          <a href="https://arxiv.org/abs/2506.01348" target="_blank" rel="noopener">arXiv</a> /
          <a href="https://github.com/noc-lab/drl_cox" target="_blank" rel="noopener">code</a>
        </div>
      </div>
    </section>

    <div class="footer">© <span id="year"></span> Yeping Jin</div>
  </div>

  <script>document.getElementById('year').textContent = new Date().getFullYear()</script>
</body>
</html>
